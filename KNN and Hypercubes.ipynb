{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e85b9d-2d5d-40f3-8492-5320a03ecf0a",
   "metadata": {},
   "source": [
    "# KNN and Hypercubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c8b53-e2d8-49ba-aeec-cac604256c23",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3270f58-0ab8-422e-9692-08097a99289f",
   "metadata": {},
   "source": [
    "When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5facc620-7256-42e0-9648-5fcf4ce039bd",
   "metadata": {},
   "source": [
    "### Exercise 4a:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256c00e-3cc1-42f0-85f8-cd1fa9bcefc8",
   "metadata": {},
   "source": [
    "Suppose that we have a set of observations, each with measurements on p = 1 feature, X. We assume that X is uniformly (evenly) distributed on [0,1]. Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10% of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with X = 0.6, we will use observations in the range [0.55, 0.65]. On average, what fraction of the available observations will we use to make the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92e44d-40ac-4ad5-b4e0-3e953d2f9393",
   "metadata": {},
   "source": [
    "<span style=\"color: blue;\">In one dimension, the window we consider is 10% of the range [0,1], which has length 0.1. So, the fraction of observations we use is 0.1, or 10%.<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c587ec-fade-4201-a191-d2bf7e0722de",
   "metadata": {},
   "source": [
    "### Exercise 4b:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059558e-9d10-4269-b2ef-8964e0f47b5a",
   "metadata": {},
   "source": [
    "Now suppose that we have a set of observations, each with measurements on p = 2 features, X1 and X2. We assume that (X1,X2) are uniformly distributed on [0, 1] × [0, 1]. We wish to predict a test observation’s response using only observations that are within 10% of the range of X1 and within 10% of the range of X2 closest to that test observation. For instance, in order to predict the response for a test observation with X1 = 0.6 and X2 = 0.35, we will use observations in the range [0.55, 0.65] for X1 and in the range [0.3, 0.4] for X2. On average, what fraction of the available observations will we use to make the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69562242-a78b-47cc-88be-8b8673b534e0",
   "metadata": {},
   "source": [
    "<span style=\"color: blue;\">In two dimensions, the region around the test point is a square with side length 0.1.\n",
    "\n",
    "<span style=\"color: blue;\">The area of this square is 0.1 × 0.1 = 0.01.\n",
    "\n",
    "<span style=\"color: blue;\">So, we use 0.01, or 1% of the observations.<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3c88b-8ca4-4b35-a40b-11a800941b11",
   "metadata": {},
   "source": [
    "### Exercise 4c:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd68a405-3ebe-41b2-a3ad-8da018bddb6a",
   "metadata": {},
   "source": [
    "Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10% of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8481ded-67b6-447a-83c0-41d01932af54",
   "metadata": {},
   "source": [
    "<span style=\"color: blue;\">In 100 dimensions, the region is a hypercube with side length 0.1.\n",
    "\n",
    "<span style=\"color: blue;\">The volume of this hypercube is (0.1)^100 = 10^(-100).<span>\n",
    "\n",
    "<span style=\"color: blue;\">This fraction is really close to 0, so almost no observations are used.<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca59464-bfa2-4301-a4da-3544c517354d",
   "metadata": {},
   "source": [
    "### Exercise 4d:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4afa518-c8ed-4c28-a449-2aedba750b38",
   "metadata": {},
   "source": [
    "Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training observations “near” any given test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd36d625-2e20-401d-af29-7ed407e0d932",
   "metadata": {},
   "source": [
    "<span style=\"color: blue;\">From parts (a)–(c), we see that the fraction of data used is (0.1)^p.\n",
    "As p increases, (0.1)^p becomes exponentially smaller.\n",
    "This means that in high dimensions, there are basically no training observations near the test observation, which makes KNN and other local methods perform badly.<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa197e6a-b506-40b6-aabf-5ee356e472e5",
   "metadata": {},
   "source": [
    "### Exercise 4e:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18194121-74b7-4536-ae45-90916246a48c",
   "metadata": {},
   "source": [
    "Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For p = 1, 2, and 100, what is the length of each side of the hypercube? Comment on your answer. Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When p = 1, a hypercube is simply a line segment, when p = 2 it is a square, and when p = 100 it is a 100-dimensional cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3113a7f-8fa6-40e2-8316-d74efef57a76",
   "metadata": {},
   "source": [
    "<span style=\"color: blue;\">\n",
    "    \n",
    "If the region has to contain 10% of the data, then the side length s of the hypercube must be:\n",
    "\n",
    "s^p = 0.1, so s = 0.1^(1/p).\n",
    "\n",
    "For p = 1: s = 0.1^(1/1) = 0.1.\n",
    "\n",
    "For p = 2: s = 0.1^(1/2) = √0.1 ≈ 0.316.\n",
    "\n",
    "For p = 100: s = 0.1^(1/100) ≈ 0.977.\n",
    "\n",
    "This shows that in one dimension the local neighborhood is small, but in 100 dimensions the side length has be about 0.98, which means that the hypercube covers almost the entire range of every feature. <span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
